{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xxP2w6jcuk7"
      },
      "source": [
        "Copyright 2022 DeepMind Technologies Limited\n",
        "\n",
        "All software is licensed under the Apache License, Version 2.0 (Apache 2.0);\n",
        "you may not use this file except in compliance with the Apache 2.0 license.\n",
        "You may obtain a copy of the Apache 2.0 license at:\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "All other materials are licensed under the Creative Commons Attribution 4.0\n",
        "International License (CC-BY). You may obtain a copy of the CC-BY license at:\n",
        "https://creativecommons.org/licenses/by/4.0/legalcode\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, all software and\n",
        "materials distributed here under the Apache 2.0 or CC-BY licenses are\n",
        "distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,\n",
        "either express or implied. See the licenses for the specific language governing\n",
        "permissions and limitations under those licenses.\n",
        "\n",
        "This is not an official Google product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d88AJSu2dBDr"
      },
      "source": [
        "# **Causal analysis of agents using the Agent Debugger**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9rnDfPOyv9V"
      },
      "source": [
        "This colab contains the experiments presented in the paper [Causal Analysis of Agent Behavior for AI Safety](https://arxiv.org/pdf/2103.03938.pdf).\n",
        "\n",
        "It uses agents which have been previously trained using [Impala](https://arxiv.org/pdf/1802.01561.pdf), and we download their parameters (i.e. neural networks weights) from local files, also open sourced. The environments are based on [Pycolab](https://github.com/deepmind/pycolab), an open source library to build 2D gridworlds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G_B4ySezog7"
      },
      "source": [
        "The main tool used is the \"Agent Debugger\", which allows us to perform interventions easily and in a standardized way on the agent and the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D07vGGnMQ4tc"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/deepmind/agent_debugger.git\n",
        "!pip install -r agent_debugger/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VK2RNVV2lCmo"
      },
      "outputs": [],
      "source": [
        "# python3\n",
        "# @title Imports\n",
        "\n",
        "import collections\n",
        "import dill\n",
        "import functools\n",
        "import itertools\n",
        "import random\n",
        "import requests\n",
        "from typing import Any\n",
        "\n",
        "import dm_env\n",
        "import haiku as hk\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from agent_debugger.src import impala_net\n",
        "from agent_debugger.src import impala_agent\n",
        "\n",
        "from agent_debugger.src.agent_debugger import node as node_lib\n",
        "from agent_debugger.src.agent_debugger.pycoworld import debugger as pcw_dbg\n",
        "from agent_debugger.src.agent_debugger.pycoworld import interventions as pcw_interv\n",
        "from agent_debugger.src.pycoworld import default_constants as pcw_constants\n",
        "from agent_debugger.src.pycoworld import environment as pcw_env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pJhjvVVRszr5"
      },
      "outputs": [],
      "source": [
        "# @title Constants used throughout.\n",
        "\n",
        "n_rollouts = 100\n",
        "\n",
        "# Useful shortcut.\n",
        "Tile = pcw_constants.Tile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WTQznPzM6pef"
      },
      "outputs": [],
      "source": [
        "# @title Utils used throughout.\n",
        "\n",
        "def prepare_init_nodes(debugger: pcw_dbg.PycoworldDebugger) -\u003e list[node_lib.Node]:\n",
        "  \"\"\"Returns a list of nodes, which agent and env seeds have been changed.\"\"\"\n",
        "  root_node = debugger.get_root_node()\n",
        "  init_nodes = []\n",
        "  for seed in range(n_rollouts):\n",
        "    node = debugger.interventions.change_env_seed(root_node, seed)\n",
        "    node = debugger.interventions.change_agent_seed(node, 2 * seed)\n",
        "    init_nodes.append(node)\n",
        "  return init_nodes\n",
        "\n",
        "def load_params(agent_name: str) -\u003e Any:\n",
        "  \"\"\"Returns a set of parameters downloaded from google storage.\"\"\"\n",
        "  url_base = \"https://storage.googleapis.com/dm_agent_debugger/{}.dill\"\n",
        "  url = url_base.format(agent_name)\n",
        "  file = requests.get(url, allow_redirects=True, stream=True)\n",
        "  return dill.load(file.raw)\n",
        "\n",
        "def make_agent_from_params(\n",
        "    params: hk.Params, with_lstm: bool\n",
        ") -\u003e impala_agent.ImpalaAgent:\n",
        "  \"\"\"Returns an Impala agent, adapted for the Agent Debugger.\n",
        "\n",
        "  Args:\n",
        "    params: The parameters of the agent (neural network weights).\n",
        "    with_lstm: Whether the agent contains an LSTM cell or not.\n",
        "  \"\"\"\n",
        "  net_factory = functools.partial(\n",
        "      impala_net.RecurrentConvNet,\n",
        "      conv_widths=(128, 128, 128),\n",
        "      conv_kernels=(3, 3, 3),\n",
        "      padding='SAME',\n",
        "      torso_widths=(128,),\n",
        "      lstm_width=128 if with_lstm else 0,\n",
        "      head_widths=(128,),\n",
        "      num_actions=4\n",
        "  )\n",
        "\n",
        "  # Changing the keys of the parameters to match the module's name.\n",
        "  new_params = {}\n",
        "  for key in params:\n",
        "    values = key.split('/')\n",
        "    new_key = '/'.join(['recurrent_conv_net']+values[1:])\n",
        "    new_params[new_key] = params[key]\n",
        "\n",
        "  return impala_agent.ImpalaAgent(net_factory=net_factory, params=new_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RQgTCHKkwbM"
      },
      "source": [
        "## 1 - Confounders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lCEGWaVq5ZUz"
      },
      "outputs": [],
      "source": [
        "# @title Create interventions.\n",
        "# @markdown An intervention acts on a node (see node.py in the code) and changes some internal states (agent or environment).\n",
        "# @markdown In this case we change the floor type and pill position, which are attributes of the environment.\n",
        "\n",
        "def change_floor_type(node: node_lib.Node, floor_type: str) -\u003e node_lib.Node:\n",
        "  new_floor_type = Tile.SAND if floor_type==\"sand\" else Tile.GRASS\n",
        "  # These positions are hardcoded for the environment we use, ie \"grass_sand\".\n",
        "  for position in [(3, 2), (3, 3), (3, 4), (2, 4), (4, 4)]:\n",
        "    node = debugger.interventions.replace_backdrop_element(\n",
        "        node, position=position, new_element_id=new_floor_type)\n",
        "  return node\n",
        "\n",
        "def change_pill_position(node: node_lib.Node, pill_pos: str) -\u003e node_lib.Node:\n",
        "  new_position = (1, 4) if pill_pos==\"right\" else (5, 4)\n",
        "  position = debugger.extractors.get_element_positions(node, element_id=Tile.REWARD)[0]\n",
        "  # Check that the position is actually new, otherwise there is nothing to do.\n",
        "  if new_position != position:\n",
        "    node = debugger.interventions.move_drape_element_to(\n",
        "        node, drape_id=Tile.REWARD, start_position=position, dest_position=new_position)\n",
        "    # Don't forget to change the terminal states too, which lie underneath the tiles.\n",
        "    node = debugger.interventions.replace_backdrop_element(node, new_position, Tile.TERMINAL)\n",
        "    floor_type = debugger.extractors.get_backdrop_curtain(node)[3, 2]\n",
        "    node = debugger.interventions.replace_backdrop_element(node, position, floor_type)\n",
        "  return node\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qz8JvK1A5aW9"
      },
      "outputs": [],
      "source": [
        "# @title Create variable extractors.\n",
        "# @markdown This tells us what is the reward's postion or the agent's position for a given rollout. We use this information to compute the probabilities.\n",
        "\n",
        "def get_agent_final_position(rollout: list[node_lib.Node]) -\u003e str:\n",
        "  \"\"\"Returns the agent final position, in {\"right\", \"left\"}.\"\"\"\n",
        "  final_position = debugger.extractors.get_element_positions(\n",
        "      rollout[-1], Tile.PLAYER)[0]\n",
        "  return \"right\" if final_position[0] \u003c= 2 else \"left\"\n",
        "\n",
        "def get_reward_position(rollout: list[node_lib.Node]) -\u003e str:\n",
        "  \"\"\"Returns the reward position, in {\"right\", \"left\"}.\"\"\"\n",
        "  reward_position = debugger.extractors.get_element_positions(\n",
        "      rollout[0], Tile.REWARD)[0]\n",
        "  return \"right\" if reward_position[0] \u003c= 2 else \"left\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Vi6Qu3pbaeZ3"
      },
      "outputs": [],
      "source": [
        "# @title Create the debugger for our agent and environment.\n",
        "# @markdown A different agent implies a different debugger, because it acts on the coupled agent+environment dynamical system.\n",
        "\n",
        "agent = \"B\" #@param[\"A\", \"B\"]\n",
        "\n",
        "# Agent A - trained on correlated reward+floor\n",
        "if agent == \"A\":\n",
        "  params = load_params('confounders_a')\n",
        "  env = pcw_env.build_environment(level='grass_sand')\n",
        "\n",
        "# Agent B - trained on uncorrelated reward+floor\n",
        "if agent == \"B\":\n",
        "  params = load_params('confounders_b')\n",
        "  env = pcw_env.build_environment(level='grass_sand_uncorrelated')\n",
        "\n",
        "trained_agent = make_agent_from_params(params, with_lstm=True)\n",
        "\n",
        "debugger = pcw_dbg.PycoworldDebugger(trained_agent, env)\n",
        "root_node = debugger.get_root_node()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-N4jtCqPti8f"
      },
      "outputs": [],
      "source": [
        "# @title Prepare nodes with different agent and environment seeds.\n",
        "# @markdown This cell creates nodes with different environment and agent seeds. We'll use these nodes later, but intervene on them before creating the rollouts.\n",
        "\n",
        "init_nodes = prepare_init_nodes(debugger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0oPoYj27updg"
      },
      "outputs": [],
      "source": [
        "# @title Compute conditional probabilities.\n",
        "# @markdown \u003cstrong\u003eNote that, in all the colab, we compute the conditional probabilities using P(A|B) = P(A and B) / P(B).\u003c/strong\u003e\n",
        "\n",
        "reward_position_count = collections.defaultdict(lambda: 0)\n",
        "reward_match_agent_count = collections.defaultdict(lambda: 0)\n",
        "for node in init_nodes:\n",
        "  rollout = debugger.get_rollout(node, maximum_length=15)\n",
        "  reward_position = get_reward_position(rollout)\n",
        "  agent_final_position = get_agent_final_position(rollout)\n",
        "  reward_position_count[reward_position] += 1\n",
        "  if reward_position == agent_final_position:\n",
        "    reward_match_agent_count[reward_position] += 1\n",
        "print(\"P(T=l | R=l) = \" + str(reward_match_agent_count[\"left\"] / (reward_position_count[\"left\"])))\n",
        "print(\"P(T=r | R=r) = \" + str(reward_match_agent_count[\"right\"] / (reward_position_count[\"right\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ASVK7QeN0wfr"
      },
      "outputs": [],
      "source": [
        "# @title Compute interventional probabilities - reward position interventions.\n",
        "\n",
        "agent_right_count = 0\n",
        "for node in init_nodes:\n",
        "  do_reward_right = change_pill_position(node, pill_pos='right')\n",
        "  rollout = debugger.get_rollout(do_reward_right, maximum_length=15)\n",
        "  agent_right_count += int(get_agent_final_position(rollout) == 'right')\n",
        "print(\"P(T=r | do(R=r)) = \" + str(agent_right_count / (len(init_nodes))))\n",
        "\n",
        "agent_left_count = 0\n",
        "for node in init_nodes:\n",
        "  do_reward_left = change_pill_position(node, pill_pos='left')\n",
        "  rollout = debugger.get_rollout(do_reward_left, maximum_length=15)\n",
        "  agent_left_count += int(get_agent_final_position(rollout) == 'left')\n",
        "print(\"P(T=l | do(R=l)) = \" + str(agent_left_count / (len(init_nodes))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "g3uA3Na71cUV"
      },
      "outputs": [],
      "source": [
        "# @title Compute interventional probabilities - floor type interventions.\n",
        "\n",
        "agent_right_count = 0\n",
        "for node in init_nodes:\n",
        "  do_floor_sand = change_floor_type(node, \"sand\")\n",
        "  rollout = debugger.get_rollout(do_floor_sand, maximum_length=15)\n",
        "  agent_right_count += int(get_agent_final_position(rollout) == 'right')\n",
        "print(\"P(T=r | do(F=s)) = \" + str(agent_right_count / (len(init_nodes))))\n",
        "\n",
        "agent_left_count = 0\n",
        "for node in init_nodes:\n",
        "  do_floor_grass = change_floor_type(node, \"grass\")\n",
        "  rollout = debugger.get_rollout(do_floor_grass, maximum_length=15)\n",
        "  agent_left_count += int(get_agent_final_position(rollout) == 'left')\n",
        "print(\"P(T=l | do(F=g)) = \" + str(agent_left_count / (len(init_nodes))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQaEwvB83OhV"
      },
      "source": [
        "# 2 - Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XRY0rWWTsHmb"
      },
      "outputs": [],
      "source": [
        "# @title Create variable extractors.\n",
        "\n",
        "def floor_type(rollout):\n",
        "  is_sand = bool(debugger.extractors.get_element_positions(rollout[0], Tile.SAND))\n",
        "  return 'sand' if is_sand else 'grass'\n",
        "\n",
        "def agent_pos_after_interv(rollout):\n",
        "  pos = debugger.extractors.get_element_positions(rollout[5], Tile.PLAYER)[0]\n",
        "  return 'right' if pos.row \u003c= 3 else 'left'\n",
        "\n",
        "def agent_pos_end(rollout):\n",
        "  pos = debugger.extractors.get_element_positions(rollout[-1], Tile.PLAYER)[0]\n",
        "  return 'right' if pos.row \u003c= 3 else 'left'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5r-EIby03Rq3"
      },
      "outputs": [],
      "source": [
        "# @title Create the debugger.\n",
        "\n",
        "agent = \"B\" #@param[\"A\", \"B\"]\n",
        "\n",
        "env = pcw_env.build_environment(\n",
        "    level='large_color_memory',\n",
        "    egocentric_horizon=1\n",
        ")\n",
        "\n",
        "# Agent A - with memory\n",
        "if agent == \"A\":\n",
        "  params = load_params('memory_a')\n",
        "  with_lstm = True\n",
        "\n",
        "# Agent B - without memory\n",
        "if agent == \"B\":\n",
        "  params = load_params('memory_b')\n",
        "  with_lstm = False\n",
        "\n",
        "trained_agent = make_agent_from_params(params, with_lstm=with_lstm)\n",
        "\n",
        "debugger = pcw_dbg.PycoworldDebugger(trained_agent, env)\n",
        "init_nodes = prepare_init_nodes(debugger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N7OTQP0Jr3n1"
      },
      "outputs": [],
      "source": [
        "# @title Compute conditional probabilities.\n",
        "\n",
        "floor_type_l = []\n",
        "agent_pos_after_interv_l = []\n",
        "agent_pos_end_l = []\n",
        "for node in init_nodes:\n",
        "  rollout = debugger.get_rollout(node, maximum_length=15)\n",
        "  floor_type_l.append(floor_type(rollout))\n",
        "  agent_pos_after_interv_l.append(agent_pos_after_interv(rollout))\n",
        "  agent_pos_end_l.append(agent_pos_end(rollout))\n",
        "\n",
        "print(\"P(T=l | F=g) = \"+str(np.mean(np.logical_and(np.array(agent_pos_end_l)==\"left\", np.array(floor_type_l)==\"grass\"))/np.mean(np.array(floor_type_l)==\"grass\")))\n",
        "print(\"P(T=r | F=s) = \"+str(np.mean(np.logical_and(np.array(agent_pos_end_l)==\"right\", np.array(floor_type_l)==\"sand\"))/np.mean(np.array(floor_type_l)==\"sand\")))\n",
        "print(\"P(P=l | F=g) = \"+str(np.mean(np.logical_and(np.array(agent_pos_after_interv_l)==\"left\", np.array(floor_type_l)==\"grass\"))/np.mean(np.array(floor_type_l)==\"grass\")))\n",
        "print(\"P(P=r | F=s) = \"+str(np.mean(np.logical_and(np.array(agent_pos_after_interv_l)==\"right\", np.array(floor_type_l)==\"sand\"))/np.mean(np.array(floor_type_l)==\"sand\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gNcBgnHkuVXe"
      },
      "outputs": [],
      "source": [
        "# @title Compute interventional probabilities.\n",
        "\n",
        "UP = 0\n",
        "DOWN = 1\n",
        "LEFT = 2\n",
        "RIGHT = 3\n",
        "\n",
        "floor_type_l = []\n",
        "agent_pos_end_l = []\n",
        "for node in init_nodes:\n",
        "  breakpoint = lambda node: node.episode_step == 3\n",
        "  intervention_at_breakpoint = functools.partial(\n",
        "      debugger.interventions.change_agent_next_actions,\n",
        "      forced_next_actions=([UP, UP]))\n",
        "  rollout = debugger.get_intervened_rollout(\n",
        "      node, 15,\n",
        "      breakpoint, intervention_at_breakpoint)\n",
        "  floor_type_l.append(floor_type(rollout))\n",
        "  agent_pos_end_l.append(agent_pos_end(rollout))\n",
        "\n",
        "print(\"P(T=l | do(P=r), F=g) = \"+str(np.mean(np.logical_and(np.array(agent_pos_end_l)==\"left\", np.array(floor_type_l)==\"grass\"))/np.mean(np.array(floor_type_l)==\"grass\")))\n",
        "\n",
        "\n",
        "floor_type_l = []\n",
        "agent_pos_end_l = []\n",
        "for node in init_nodes:\n",
        "  breakpoint = lambda node: node.episode_step == 3\n",
        "  intervention_at_breakpoint = functools.partial(\n",
        "      debugger.interventions.change_agent_next_actions,\n",
        "      forced_next_actions=([DOWN, DOWN]))\n",
        "  rollout = debugger.get_intervened_rollout(\n",
        "      node, 15,\n",
        "      breakpoint, intervention_at_breakpoint)\n",
        "  floor_type_l.append(floor_type(rollout))\n",
        "  agent_pos_end_l.append(agent_pos_end(rollout))\n",
        "\n",
        "print(\"P(T=r | do(P=l), F=s) = \"+str(np.mean(np.logical_and(np.array(agent_pos_end_l)==\"right\", np.array(floor_type_l)==\"sand\"))/np.mean(np.array(floor_type_l)==\"sand\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAOYm9E_5MMN"
      },
      "source": [
        "# 3 - Robust generalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "recDkNO25lao"
      },
      "outputs": [],
      "source": [
        "# @title Create interventions and variable extractors.\n",
        "\n",
        "def move_reward_to_quadrant(node, quadrant: str):\n",
        "  all_positions = itertools.product(range(1, 7), range(1, 7))\n",
        "  if quadrant == 'south':\n",
        "    filter_quadrant = lambda pos: pos[0] \u003e= 4 and pos[1] \u003e= 4\n",
        "  elif quadrant == 'north':\n",
        "    filter_quadrant = lambda pos: pos[0] \u003c 4 and pos[1] \u003c 4\n",
        "  elif quadrant == 'east':\n",
        "    filter_quadrant = lambda pos: pos[0] \u003c 4 and pos[1] \u003e= 4\n",
        "  else:\n",
        "    filter_quadrant = lambda pos: pos[0] \u003e= 4 and pos[1] \u003c 4\n",
        "  positions = list(filter(filter_quadrant, all_positions))\n",
        "  agent_pos = debugger.extractors.get_element_positions(node, Tile.PLAYER)[0]\n",
        "  agent_pos = (agent_pos.row, agent_pos.col)\n",
        "  if agent_pos in positions:\n",
        "    positions.remove(agent_pos)\n",
        "  new_reward_pos = random.choice(positions)\n",
        "  reward_pos = debugger.extractors.get_element_positions(node, Tile.REWARD)[0]\n",
        "  node = debugger.interventions.move_drape_element_to(\n",
        "      node, Tile.REWARD, start_position=reward_pos, dest_position=new_reward_pos)\n",
        "  node = debugger.interventions.replace_backdrop_element(node, reward_pos, Tile.FLOOR)\n",
        "  node = debugger.interventions.replace_backdrop_element(node, new_reward_pos, Tile.TERMINAL_R)\n",
        "  return node\n",
        "\n",
        "def reward_taken(rollout):\n",
        "  return (rollout[-1].last_timestep.reward == 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Vk1WlWHd5UvK"
      },
      "outputs": [],
      "source": [
        "# @title Create the debugger\n",
        "\n",
        "agent = \"B\" #@param[\"A\", \"B\"]\n",
        "\n",
        "# Agent A - trained on the full environment\n",
        "if agent == \"A\":\n",
        "  params = load_params('generalization_a')\n",
        "  env = pcw_env.build_environment(level='apples_full')\n",
        "\n",
        "# Agent B - trained only on part of the environment distribution\n",
        "if agent == \"B\":\n",
        "  params = load_params('generalization_b')\n",
        "  env = pcw_env.build_environment(level='apples_corner')\n",
        "\n",
        "trained_agent = make_agent_from_params(params, with_lstm=True)\n",
        "\n",
        "debugger = pcw_dbg.PycoworldDebugger(trained_agent, env)\n",
        "init_nodes = prepare_init_nodes(debugger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "46ppkAQf8VAL"
      },
      "outputs": [],
      "source": [
        "# @title Compute conditional probs.\n",
        "\n",
        "reward_taken_count = 0\n",
        "for node in init_nodes:\n",
        "  rollout = debugger.get_rollout(node, maximum_length=15)\n",
        "  reward_taken_count += int(reward_taken(rollout))\n",
        "print(\"P(R=l) = \" + str(reward_taken_count / len(init_nodes)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uYagr17BweZ5"
      },
      "outputs": [],
      "source": [
        "# @title Compute interventional probs.\n",
        "\n",
        "for quadrant in ['south', 'north', 'east', 'west']:\n",
        "  reward_taken_count = 0\n",
        "  for node in init_nodes:\n",
        "    node = move_reward_to_quadrant(node, quadrant)\n",
        "    rollout = debugger.get_rollout(node, maximum_length=15)\n",
        "    reward_taken_count += int(reward_taken(rollout))\n",
        "  print(\"P(R=1 | G=\"+quadrant+\") = \" + str(reward_taken_count / len(init_nodes)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMne5HKZxy8f"
      },
      "source": [
        "# 4 - Counterfactuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cjOe_6lr4ReZ"
      },
      "outputs": [],
      "source": [
        "# @title Create interventions and variable extractors.\n",
        "\n",
        "def move_door_to(node, door_pos: str):\n",
        "  new_position = (3, 1) if door_pos == \"left\" else (3, 5)\n",
        "  position = debugger.extractors.get_element_positions(node, Tile.DOOR_B)[0]\n",
        "  return debugger.interventions.move_drape_element_to(\n",
        "      node, Tile.DOOR_B, start_position=position, dest_position=new_position)\n",
        "  \n",
        "def reward_taken(rollout):\n",
        "  agent_pos = debugger.extractors.get_element_positions(\n",
        "      rollout[-1], Tile.PLAYER)[0]\n",
        "  if agent_pos in [(2, 2), (2, 4)]:\n",
        "    return \"green\"\n",
        "  return \"red\"\n",
        "\n",
        "def door_position(node):\n",
        "  door_pos = debugger.extractors.get_element_positions(rollout[0], Tile.DOOR_B)[0]\n",
        "  door_pos = (door_pos.row, door_pos.col)\n",
        "  return \"left\" if door_pos == (3, 1) else \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vFQx-ZZsx2kQ"
      },
      "outputs": [],
      "source": [
        "# @title Compute variable values for both agents.\n",
        "# @markdown We compute the probabilities slightly differently than before here since we consider probabilities involving the ID of the agent (variable 'A' in the paper), and therefore compute values which depend on the metrics for agents 're' and 'gr' simultaneously.\n",
        "\n",
        "reward_taken_l = {\"A\": [], \"B\": []}\n",
        "door_pos_l = {\"A\": [], \"B\": []}\n",
        "\n",
        "for (agent_id, agent_name) in [(\"A\", \"counterfactuals_a\"), (\"B\", \"counterfactuals_b\")]:\n",
        "  env = pcw_env.build_environment(level='red_green_apples')\n",
        "\n",
        "  params = load_params(agent_name)\n",
        "  trained_agent = make_agent_from_params(params, with_lstm=True)\n",
        "\n",
        "  debugger = pcw_dbg.PycoworldDebugger(trained_agent, env)\n",
        "  init_nodes = prepare_init_nodes(debugger)\n",
        "  \n",
        "  # Conditional regime.\n",
        "  for node in init_nodes:\n",
        "    rollout = debugger.get_rollout(node, maximum_length=15)\n",
        "    reward_taken_l[agent_id].append(reward_taken(rollout))\n",
        "    door_pos_l[agent_id].append(door_position(rollout))\n",
        "  \n",
        "  # Interventional regime.\n",
        "  reward_taken_count = collections.defaultdict(lambda: 0)\n",
        "  for node in init_nodes:\n",
        "    node = move_door_to(node, \"right\")\n",
        "    rollout = debugger.get_rollout(node, maximum_length=15)\n",
        "    reward_taken_count[reward_taken(rollout)] += 1\n",
        "  if agent_id == \"A\":\n",
        "    print(\"P(R_{D=r}=gr | D=l, R=gr) = \" + str(reward_taken_count[\"green\"] / len(init_nodes)))\n",
        "  if agent_id == \"B\":\n",
        "    print(\"P(R_{D=r}=re | D=l, R=re) = \" + str(reward_taken_count[\"red\"] / len(init_nodes)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Pw6o9hSS5NKs"
      },
      "outputs": [],
      "source": [
        "# @title Compute counterfactual probs.\n",
        "\n",
        "reward_a_red = np.array(reward_taken_l[\"A\"])==\"red\"\n",
        "reward_b_red = np.array(reward_taken_l[\"B\"])==\"red\"\n",
        "door_b_left = np.array(door_pos_l[\"B\"])==\"left\"\n",
        "door_a_left = np.array(door_pos_l[\"A\"])==\"left\"\n",
        "\n",
        "print(\"P(R=re) = \"+str((np.sum(reward_a_red) + np.sum(reward_b_red))/(2*n_rollouts)))\n",
        "print(\"P(A=re | R=re) = \"+str(np.mean(reward_b_red)/np.mean(reward_a_red + reward_b_red)))\n",
        "\n",
        "p_intersect = np.mean(np.logical_and(reward_b_red, door_b_left))\n",
        "p_evidence = np.mean(np.logical_and(reward_a_red, door_a_left) + np.logical_and(reward_b_red, door_b_left))\n",
        "print(\"P(A=re | D=l, R=re) = \"+str(p_intersect / p_evidence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbvBgKlf70CH"
      },
      "source": [
        "# 5 - Causal induction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jcl2r4_k9qOi"
      },
      "source": [
        "No need to use pycoworld here, as the setup is very simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eggfoUCv9dh4"
      },
      "outputs": [],
      "source": [
        "# @title Creating leader and follower actions, no interventions.\n",
        "\n",
        "episode_length = 1\n",
        "n_episodes = 10000\n",
        "\n",
        "# Agent 1 is red, 2 is blue.\n",
        "# Action 0 is right, action 1 is left.\n",
        "agent_1_actions, agent_2_actions = [], []\n",
        "leader_bool = []\n",
        "for episode in range(n_episodes):\n",
        "  leader = np.random.randint(0, 2)\n",
        "  leader_bool.append(leader)\n",
        "\n",
        "  leader_action = np.random.randint(0, 2)\n",
        "  follower_action = leader_action\n",
        "  if np.random.rand() \u003c 0.1:\n",
        "    follower_action = np.random.randint(0, 2)\n",
        "  if leader == 0:\n",
        "    agent_1_actions.append(leader_action)\n",
        "    agent_2_actions.append(follower_action)\n",
        "  else:\n",
        "    agent_1_actions.append(follower_action)\n",
        "    agent_2_actions.append(leader_action)\n",
        "leader_bool = np.array(leader_bool)\n",
        "agent_1_actions = np.array(agent_1_actions)\n",
        "agent_2_actions = np.array(agent_2_actions)\n",
        "\n",
        "print(\"P(L=b) = \"+str(np.mean(leader_bool)))\n",
        "print(\"P(L=b | R=l, B=l) = \"+str(np.mean(leader_bool * agent_1_actions * agent_2_actions) / np.mean(agent_1_actions * agent_2_actions)))\n",
        "print(\"P(L=b | R=l, B=r) = \"+str(np.mean(leader_bool * agent_1_actions * (1-agent_2_actions)) / np.mean(agent_1_actions * (1-agent_2_actions))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nRg4gLxcBS4x"
      },
      "outputs": [],
      "source": [
        "# @title Creating leader and follower actions, with intervention do(R=r)\n",
        "\n",
        "forced_action = 0\n",
        "agent_1_actions, agent_2_actions = [], []\n",
        "leader_bool = []\n",
        "for episode in range(n_episodes):\n",
        "  leader = np.random.randint(0, 2)\n",
        "  leader_bool.append(leader)\n",
        "\n",
        "  agent_1_actions.append(forced_action)\n",
        "  leader_action = np.random.randint(0, 2) if leader == 1 else forced_action\n",
        "  if leader == 1:\n",
        "    agent_2_actions.append(leader_action)\n",
        "  else:\n",
        "    follower_action = leader_action\n",
        "    if np.random.rand() \u003c 0.1:\n",
        "      follower_action = np.random.randint(0, 2)\n",
        "    agent_2_actions.append(follower_action)\n",
        "leader_bool = np.array(leader_bool)\n",
        "agent_1_actions = np.array(agent_1_actions)\n",
        "agent_2_actions = np.array(agent_2_actions)\n",
        "\n",
        "print(\"P(L=b | do(R=r), B=l) = \"+str(np.mean(leader_bool * agent_2_actions) / np.mean(agent_2_actions)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-iZ6Pk6RGY4g"
      },
      "outputs": [],
      "source": [
        "# @title Creating leader and follower actions, with intervention do(R=l)\n",
        "\n",
        "forced_action = 1\n",
        "agent_1_actions, agent_2_actions = [], []\n",
        "leader_bool = []\n",
        "for episode in range(n_episodes):\n",
        "  leader = np.random.randint(0, 2)\n",
        "  leader_bool.append(leader)\n",
        "\n",
        "  agent_1_actions.append(forced_action)\n",
        "  leader_action = np.random.randint(0, 2) if leader == 1 else forced_action\n",
        "  if leader == 1:\n",
        "    agent_2_actions.append(leader_action)\n",
        "  else:\n",
        "    follower_action = leader_action\n",
        "    if np.random.rand() \u003c 0.1:\n",
        "      follower_action = np.random.randint(0, 2)\n",
        "    agent_2_actions.append(follower_action)\n",
        "leader_bool = np.array(leader_bool)\n",
        "agent_1_actions = np.array(agent_1_actions)\n",
        "agent_2_actions = np.array(agent_2_actions)\n",
        "\n",
        "print(\"P(L=b | do(R=l), B=l) = \"+str(np.mean(leader_bool * agent_2_actions) / np.mean(agent_2_actions)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9dojnwUHdfI"
      },
      "source": [
        "# 6 - Causal pathways"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Yaf0uSsMKIbc"
      },
      "outputs": [],
      "source": [
        "# @title Create interventions.\n",
        "\n",
        "def change_door_state(node, open: bool):\n",
        "  door_pos = debugger.extractors.get_element_positions(node, Tile.DOOR_R)\n",
        "  if door_pos == [] and open == True:\n",
        "    return node\n",
        "  if door_pos != [] and open == False:\n",
        "    return node\n",
        "  \n",
        "  if open:\n",
        "    return debugger.interventions.remove_drape_element(\n",
        "        node, Tile.DOOR_R, position=(3, 4))\n",
        "  return debugger.interventions.add_drape_element(\n",
        "        node, Tile.DOOR_R, position=(3, 4))\n",
        "  \n",
        "def change_internal_key_state(node: node_lib.Node, value: int) -\u003e node_lib.Node:\n",
        "  with pcw_interv.InterventionContext(node) as context:\n",
        "    context.engine.the_plot[chr(Tile.KEY_R)] = value\n",
        "  return context.new_node\n",
        "\n",
        "def remove_key(node):\n",
        "  node = change_internal_key_state(node=node, value=0)\n",
        "  key_pos = debugger.extractors.get_element_positions(node, Tile.KEY_R)[0]\n",
        "  return debugger.interventions.remove_drape_element(\n",
        "      node, Tile.KEY_R, position=key_pos)\n",
        " \n",
        "def add_key_to_agent(node):\n",
        "  node = remove_key(node)\n",
        "  return change_internal_key_state(node, value=1)\n",
        " \n",
        "def door_state(node):\n",
        "  open = debugger.extractors.get_element_positions(rollout[0], Tile.DOOR_R) == []\n",
        "  return \"open\" if open else \"closed\"\n",
        "\n",
        "def key_taken(rollout):\n",
        "  return (debugger.extractors.get_element_positions(rollout[-1], Tile.KEY_R) == [])\n",
        "\n",
        "def reward_taken(rollout):\n",
        "  return (rollout[-1].last_timestep.reward == 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K08kGzskJoOQ"
      },
      "outputs": [],
      "source": [
        "# @title Create the debugger.\n",
        "\n",
        "agent = \"A\" #@param[\"A\", \"B\"]\n",
        "\n",
        "\n",
        "# Agent A - trained on all the environment distribution\n",
        "if agent == \"A\":\n",
        "  params = load_params('pathways_a')\n",
        "  env = pcw_env.build_environment(level='key_door')\n",
        "\n",
        "# Agent B - trained only when the door is closed\n",
        "if agent == \"B\":\n",
        "  params = load_params('pathways_b')\n",
        "  env = pcw_env.build_environment(level='key_door_closed')\n",
        "\n",
        "trained_agent = make_agent_from_params(params, with_lstm=True)\n",
        "\n",
        "debugger = pcw_dbg.PycoworldDebugger(trained_agent, env)\n",
        "init_nodes = prepare_init_nodes(debugger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jvHEdGKQIPE"
      },
      "outputs": [],
      "source": [
        "# @title Compute conditional probs.\n",
        "# @markdown Note that for agent B some probs are Nans, since the door is **always** closed in the environment, implying that the event that we condition on can have zero probability.\n",
        "\n",
        "reward_taken_l = []\n",
        "key_taken_l = []\n",
        "door_open_l = []\n",
        "for node in init_nodes:\n",
        "  rollout = debugger.get_rollout(node, maximum_length=15)\n",
        "  reward_taken_l.append(int(reward_taken(rollout)))\n",
        "  key_taken_l.append(int(key_taken(rollout)))\n",
        "  door_open_l.append(int(door_state(rollout) == \"open\"))\n",
        "key_taken_l = np.array(key_taken_l)\n",
        "reward_taken_l = np.array(reward_taken_l)\n",
        "door_open_l = np.array(door_open_l)\n",
        "\n",
        "print(\"P(R=1) = \"+str(np.mean(reward_taken_l)))\n",
        "print(\"P(R=1 | K=y) = \"+str(np.mean(reward_taken_l * key_taken_l) / np.mean(key_taken_l)))\n",
        "print(\"P(R=1 | K=n) = \"+str(np.mean(reward_taken_l * (1-key_taken_l)) / np.mean(1-key_taken_l)))\n",
        "print(\"P(R=1 | D=o) = \"+str(np.mean(reward_taken_l * door_open_l) / np.mean(door_open_l)))\n",
        "print(\"P(R=1 | D=c) = \"+str(np.mean(reward_taken_l * (1-door_open_l)) / np.mean(1-door_open_l)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VvYrXU9HRbI2"
      },
      "outputs": [],
      "source": [
        "# @title Compute interventional probs.\n",
        "\n",
        "reward_taken_l = []\n",
        "for node in init_nodes:\n",
        "  node = remove_key(node)\n",
        "  rollout = debugger.get_rollout(node, maximum_length=15)\n",
        "  reward_taken_l.append(int(reward_taken(rollout)))\n",
        "reward_taken_l = np.array(reward_taken_l)\n",
        "print(\"P(R=1 | do(K=n)) = \"+str(np.mean(reward_taken_l)))\n",
        "\n",
        "reward_taken_l = []\n",
        "for node in init_nodes:\n",
        "  node = add_key_to_agent(node)\n",
        "  rollout = debugger.get_rollout(node, maximum_length=15)\n",
        "  reward_taken_l.append(int(reward_taken(rollout)))\n",
        "reward_taken_l = np.array(reward_taken_l)\n",
        "print(\"P(R=1 | do(K=y)) = \"+str(np.mean(reward_taken_l)))\n",
        "\n",
        "key_taken_l = []\n",
        "for node in init_nodes:\n",
        "  node = change_door_state(node, open=True)\n",
        "  rollout = debugger.get_rollout(node, maximum_length=15)\n",
        "  key_taken_l.append(int(key_taken(rollout)))\n",
        "key_taken_l = np.array(key_taken_l)\n",
        "print(\"P(K=y | do(D=o)) = \"+str(np.mean(key_taken_l)))\n",
        "\n",
        "key_taken_l = []\n",
        "for node in init_nodes:\n",
        "  node = change_door_state(node, open=False)\n",
        "  rollout = debugger.get_rollout(node, maximum_length=15)\n",
        "  key_taken_l.append(int(key_taken(rollout)))\n",
        "key_taken_l = np.array(key_taken_l)\n",
        "print(\"P(K=y | do(D=c)) = \"+str(np.mean(key_taken_l)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbYAFNqGFtVN"
      },
      "source": [
        "The causal response can be calculated manually using the above."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Agent Debugger Experiments",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
